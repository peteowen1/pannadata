name: Daily Opta Scrape

on:
  schedule:
    # Run at 5 AM UTC daily (before Understat at 7 AM)
    - cron: '0 5 * * *'
  workflow_dispatch:
    inputs:
      leagues:
        description: 'Leagues to scrape (space-separated, e.g., "EPL La_Liga")'
        required: false
        type: string
      recent:
        description: 'Number of recent seasons to scrape per league'
        required: false
        type: number
        default: 1
      force_rescrape:
        description: 'Force rescrape existing matches'
        required: false
        type: boolean
        default: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout for comprehensive scrape

    permissions:
      contents: write  # Required for GitHub Releases upload

    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    steps:
    - name: Checkout pannaverse (contains theanalyst_scraper)
      uses: actions/checkout@v4
      with:
        repository: peteowen1/pannaverse
        path: pannaverse
        submodules: false

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: pannaverse/theanalyst_scraper/requirements.txt

    - name: Install Python dependencies
      run: |
        pip install -r pannaverse/theanalyst_scraper/requirements.txt

    - name: Download existing Opta data from GitHub Releases
      run: |
        # Python script writes to pannadata/data/opta/
        mkdir -p pannaverse/pannadata/data

        # Download existing opta-parquet.tar.gz if it exists
        echo "Checking for existing Opta data..."
        if gh release download opta-latest \
            --repo peteowen1/pannadata \
            --pattern "opta-parquet.tar.gz" \
            --dir . 2>/dev/null; then
          echo "Downloaded existing opta-parquet.tar.gz"
          tar -xzf opta-parquet.tar.gz -C pannaverse/pannadata/data/
          rm opta-parquet.tar.gz
          echo "Extracted existing data"
          find pannaverse/pannadata/data/opta -name "*.parquet" 2>/dev/null | wc -l | xargs -I {} echo "Existing parquet files: {}"
        else
          echo "No existing opta-parquet.tar.gz found (first run)"
          mkdir -p pannaverse/pannadata/data/opta
        fi
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Run Opta scraper
      run: |
        cd pannaverse/theanalyst_scraper

        # Build command arguments
        ARGS="--recent ${{ github.event.inputs.recent || 1 }}"

        # Add leagues if specified
        if [ -n "${{ github.event.inputs.leagues }}" ]; then
          ARGS="$ARGS --leagues ${{ github.event.inputs.leagues }}"
        fi

        # Add force flag if specified
        if [ "${{ github.event.inputs.force_rescrape }}" = "true" ]; then
          ARGS="$ARGS --force"
        fi

        echo "Running: python scrape_big5.py $ARGS"
        python scrape_big5.py $ARGS

    - name: Build consolidated Opta parquet files
      run: |
        cd pannaverse/pannadata/data

        # Count what we have
        echo "Opta directory contents:"
        find opta -type f 2>/dev/null | head -20
        echo "..."

        PARQUET_COUNT=$(find opta -name "*.parquet" 2>/dev/null | wc -l)
        echo "Total parquet files: $PARQUET_COUNT"

        if [ "$PARQUET_COUNT" -eq 0 ]; then
          echo "No parquet files to consolidate"
          exit 0
        fi

        # Create consolidated directory
        mkdir -p consolidated

        # Build consolidated parquet files using Python (pandas handles schema differences)
        cat > consolidate.py << 'PYEOF'
import os, glob, pandas as pd
opta_dir, consolidated_dir = "opta", "consolidated"
table_types = [d for d in os.listdir(opta_dir) if os.path.isdir(os.path.join(opta_dir, d))]
print(f"Found table types: {table_types}")
for table_type in table_types:
    tt_dir = os.path.join(opta_dir, table_type)
    parquet_files = glob.glob(os.path.join(tt_dir, "**/*.parquet"), recursive=True)
    if not parquet_files:
        print(f"  Skipping {table_type} - no parquet files")
        continue
    print(f"Consolidating opta_{table_type}... Found {len(parquet_files)} files")
    dfs = []
    for f in parquet_files:
        try:
            dfs.append(pd.read_parquet(f))
        except Exception as e:
            print(f"  Warning: Error reading {f}: {e}")
    if not dfs:
        continue
    combined = pd.concat(dfs, ignore_index=True)
    output_path = os.path.join(consolidated_dir, f"opta_{table_type}.parquet")
    combined.to_parquet(output_path, index=False)
    size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"  Wrote {output_path}: {len(combined):,} rows, {size_mb:.1f} MB")
print("Consolidation complete!")
PYEOF
        python3 consolidate.py

        # Show consolidated files
        echo ""
        echo "Consolidated parquet files:"
        ls -lh consolidated/*.parquet 2>/dev/null || echo "  (none)"

    - name: Create Opta parquet archive (backup)
      run: |
        cd pannaverse/pannadata/data

        PARQUET_COUNT=$(find opta -name "*.parquet" 2>/dev/null | wc -l)
        if [ "$PARQUET_COUNT" -eq 0 ]; then
          echo "No parquet files to archive"
          exit 0
        fi

        # Create tar.gz of opta directory as backup
        tar -czvf opta-parquet.tar.gz opta/

        ARCHIVE_SIZE=$(ls -lh opta-parquet.tar.gz | awk '{print $5}')
        echo "Created opta-parquet.tar.gz ($ARCHIVE_SIZE)"

    - name: Upload Opta data to GitHub Releases
      run: |
        cd pannaverse/pannadata/data

        # Create release if it doesn't exist
        if ! gh release view opta-latest --repo peteowen1/pannadata >/dev/null 2>&1; then
          echo "Creating opta-latest release..."
          gh release create opta-latest \
            --repo peteowen1/pannadata \
            --title "Opta Data (Latest)" \
            --notes "Automated weekly Opta/TheAnalyst data scrape. Individual parquet files for fast remote queries." \
            --prerelease
        fi

        # Upload individual consolidated parquet files
        echo "Uploading consolidated Opta parquet files..."
        for f in consolidated/opta_*.parquet; do
          if [ -f "$f" ]; then
            fname=$(basename "$f")
            echo "  Uploading $fname..."
            gh release upload opta-latest "$f" \
              --repo peteowen1/pannadata \
              --clobber
          fi
        done

        # Upload tar.gz file as backup
        if [ -f opta-parquet.tar.gz ]; then
          echo "Uploading opta-parquet.tar.gz (backup archive)..."
          gh release upload opta-latest opta-parquet.tar.gz \
            --repo peteowen1/pannadata \
            --clobber
        fi

        echo "Upload complete"
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Summary
      run: |
        echo "Weekly Opta scrape completed successfully"
        echo "Check pannadata repo opta-latest release for updated data"
        echo ""
        echo "Individual parquet files available for fast remote queries:"
        ls -lh pannaverse/pannadata/data/consolidated/opta_*.parquet 2>/dev/null || echo "  (none)"

        # Show scrape summary from theanalyst_scraper
        if [ -f pannaverse/theanalyst_scraper/data/scrape_summary.json ]; then
          echo ""
          echo "Scrape summary:"
          cat pannaverse/theanalyst_scraper/data/scrape_summary.json
        fi

        echo ""
        echo "Total Opta parquet files: $(find pannaverse/pannadata/data/opta -name '*.parquet' 2>/dev/null | wc -l)"
